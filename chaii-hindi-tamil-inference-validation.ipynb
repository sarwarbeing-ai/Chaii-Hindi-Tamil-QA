{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/huggingface-accelerate')\nsys.path.append('../input/transformershuggingface/transformer_repo')","metadata":{"id":"ZBKvQdjJ1_99","execution":{"iopub.status.busy":"2021-11-14T20:11:42.090877Z","iopub.execute_input":"2021-11-14T20:11:42.091199Z","iopub.status.idle":"2021-11-14T20:11:42.189426Z","shell.execute_reply.started":"2021-11-14T20:11:42.091115Z","shell.execute_reply":"2021-11-14T20:11:42.188704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint='../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'","metadata":{"id":"zCAL3UJm1_-D","execution":{"iopub.status.busy":"2021-11-14T20:11:43.144636Z","iopub.execute_input":"2021-11-14T20:11:43.145623Z","iopub.status.idle":"2021-11-14T20:11:43.150318Z","shell.execute_reply.started":"2021-11-14T20:11:43.145551Z","shell.execute_reply":"2021-11-14T20:11:43.149181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq","metadata":{"id":"ImW8Iodz1_-F","execution":{"iopub.status.busy":"2021-11-14T20:11:43.519876Z","iopub.execute_input":"2021-11-14T20:11:43.520119Z","iopub.status.idle":"2021-11-14T20:11:53.232892Z","shell.execute_reply.started":"2021-11-14T20:11:43.520094Z","shell.execute_reply":"2021-11-14T20:11:53.232021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport collections\nimport pandas as pd\nimport gc\nimport os\nimport sys\nfrom IPython.display import FileLink\n%env WANDB_DISABLED=True\nimport torch\nimport datasets as d\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader\nfrom src.accelerate import Accelerator\nimport math\nimport os\nimport random\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    default_data_collator,\n    get_scheduler,\n    SchedulerType,\n    set_seed)","metadata":{"id":"bwFxs1kk1_-F","execution":{"iopub.status.busy":"2021-11-14T20:11:53.236196Z","iopub.execute_input":"2021-11-14T20:11:53.236416Z","iopub.status.idle":"2021-11-14T20:12:03.945298Z","shell.execute_reply.started":"2021-11-14T20:11:53.236388Z","shell.execute_reply":"2021-11-14T20:12:03.944591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the seed\nseed=124\nrandom.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nset_seed(seed)","metadata":{"id":"hwRZX8p-1_-G","execution":{"iopub.status.busy":"2021-11-14T20:12:03.946737Z","iopub.execute_input":"2021-11-14T20:12:03.94698Z","iopub.status.idle":"2021-11-14T20:12:03.957606Z","shell.execute_reply.started":"2021-11-14T20:12:03.94695Z","shell.execute_reply":"2021-11-14T20:12:03.956888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load  data\nimport pandas as pd\ntrain_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nsubmission_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsquad=pd.read_csv('../input/squadhindi/xquad.csv')\nmlqa=pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')","metadata":{"id":"l5xw8bYt1_-H","execution":{"iopub.status.busy":"2021-11-14T20:12:03.959698Z","iopub.execute_input":"2021-11-14T20:12:03.960005Z","iopub.status.idle":"2021-11-14T20:12:04.891721Z","shell.execute_reply.started":"2021-11-14T20:12:03.959971Z","shell.execute_reply":"2021-11-14T20:12:04.890978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"squad['id']=['sar'+str(item) for item in range(5427,5427+squad.shape[0])]\nmlqa['id']=['sar'+str(item) for item in range(mlqa.shape[0])]\n# merge train_df and squad\ntrain_df=pd.concat([train_df,squad,mlqa])","metadata":{"id":"_2veWPZx1_-I","execution":{"iopub.status.busy":"2021-11-14T20:12:04.893131Z","iopub.execute_input":"2021-11-14T20:12:04.893385Z","iopub.status.idle":"2021-11-14T20:12:04.916977Z","shell.execute_reply.started":"2021-11-14T20:12:04.893352Z","shell.execute_reply":"2021-11-14T20:12:04.916138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_answers_as_dict(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }\n\ntrain_df['answers'] = train_df[['answer_start', 'answer_text']].apply(get_answers_as_dict, axis=1)\n\ntrain_df.reset_index(drop=True,inplace=True)","metadata":{"id":"S5iIxzW51_-K","execution":{"iopub.status.busy":"2021-11-14T20:12:04.918309Z","iopub.execute_input":"2021-11-14T20:12:04.918615Z","iopub.status.idle":"2021-11-14T20:12:05.024653Z","shell.execute_reply.started":"2021-11-14T20:12:04.918558Z","shell.execute_reply":"2021-11-14T20:12:05.023968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset into 6 folds\ntrain_df['fold']=-99\nskf=StratifiedKFold(n_splits=6,random_state=seed,shuffle=True)\nfor i,(train_index, test_index) in enumerate(skf.split(train_df, train_df['language'])):\n    train_df.loc[test_index,'fold']=i\n\n#train_df.to_csv('train_df.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T20:12:05.025881Z","iopub.execute_input":"2021-11-14T20:12:05.026223Z","iopub.status.idle":"2021-11-14T20:12:05.049608Z","shell.execute_reply.started":"2021-11-14T20:12:05.026187Z","shell.execute_reply":"2021-11-14T20:12:05.048987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper parameters\nmax_length = 384\ndoc_stride = 128\neval_batch_size = 8","metadata":{"execution":{"iopub.status.busy":"2021-11-14T20:12:05.050713Z","iopub.execute_input":"2021-11-14T20:12:05.051074Z","iopub.status.idle":"2021-11-14T20:12:05.055073Z","shell.execute_reply.started":"2021-11-14T20:12:05.051038Z","shell.execute_reply":"2021-11-14T20:12:05.054443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"id":"mVotVzf61_-L","execution":{"iopub.status.busy":"2021-11-14T20:12:05.056364Z","iopub.execute_input":"2021-11-14T20:12:05.057127Z","iopub.status.idle":"2021-11-14T20:12:05.067223Z","shell.execute_reply.started":"2021-11-14T20:12:05.057093Z","shell.execute_reply":"2021-11-14T20:12:05.066405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples,\n    features,\n    predictions,\n    n_best_size: int = 20,\n    max_answer_length: int = 30):\n    \n    assert len(predictions) == 2, \"`predictions` should be a tuple with two elements (start_logits, end_logits).\"\n    all_start_logits, all_end_logits = predictions\n\n    assert len(predictions[0]) == len(features), f\"Got {len(predictions[0])} predictions and {len(features)} features.\"\n\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    all_predictions = collections.OrderedDict()\n    \n    \n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        prelim_predictions = []\n\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n            # available in the current feature.\n            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n            \n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    # Don't consider answer that don't have the maximum context available (if such information is\n                    # provided).\n                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n                        continue\n                    prelim_predictions.append(\n                        {\n                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"start_logit\": start_logits[start_index],\n                            \"end_logit\": end_logits[end_index],\n                        }\n                    )\n        # Only keep the best `n_best_size` predictions.\n        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n\n        # Use the offsets to gather the answer text in the original context.\n        context = example[\"context\"]\n        for pred in predictions:\n            offsets = pred.pop(\"offsets\")\n            pred[\"text\"] = context[offsets[0] : offsets[1]]\n\n        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n        # failure.\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n\n        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n        # the LogSumExp trick).\n        scores = np.array([pred.pop(\"score\") for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n\n        # Include the probabilities in our predictions.\n        for prob, pred in zip(probs, predictions):\n            pred[\"probability\"] = prob\n\n        # Pick the best prediction. If the null answer is not possible, this is easy.\n        all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n    return all_predictions","metadata":{"id":"y7JGMlMf3oNz","execution":{"iopub.status.busy":"2021-11-14T20:12:05.070221Z","iopub.execute_input":"2021-11-14T20:12:05.07059Z","iopub.status.idle":"2021-11-14T20:12:05.090781Z","shell.execute_reply.started":"2021-11-14T20:12:05.070542Z","shell.execute_reply":"2021-11-14T20:12:05.08985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\ndef create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n\n        step = 0\n        # create a numpy array and fill it with -100.\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather\n        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n            # And after every iteration we have to change the step\n\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n\n            if step + batch_size < len(dataset):\n                logits_concat[step : step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n\n            step += batch_size\n\n        return logits_concat","metadata":{"id":"DwhfdjlL1_-N","execution":{"iopub.status.busy":"2021-11-14T20:12:05.091956Z","iopub.execute_input":"2021-11-14T20:12:05.092353Z","iopub.status.idle":"2021-11-14T20:12:05.102207Z","shell.execute_reply.started":"2021-11-14T20:12:05.092318Z","shell.execute_reply":"2021-11-14T20:12:05.101602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logit_predictions(test_data,checkpoint,accelerator):\n  config = AutoConfig.from_pretrained(checkpoint)\n  model = AutoModelForQuestionAnswering.from_pretrained(checkpoint,config=config)\n  device=torch.device(\"cuda\")\n  model.to(device)\n  test_dataset_for_model = test_data.remove_columns([\"example_id\", \"offset_mapping\"])\n  test_dataloader = DataLoader(\n            test_dataset_for_model, collate_fn=default_data_collator, batch_size=eval_batch_size)\n\n  all_start_logits = []\n  all_end_logits = []\n  for step, batch in enumerate(tqdm(test_dataloader)):\n        with torch.no_grad():\n            for key,value in batch.items():\n                batch[key]=value.to(device)\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n\n                \n            all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n\n  max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n  # concatenate the numpy array\n  start_logits_concat = create_and_fill_np_array(all_start_logits, test_data, max_len)\n  end_logits_concat = create_and_fill_np_array(all_end_logits, test_data, max_len)\n\n  # delete the list of numpy arrays\n  del all_start_logits\n  del all_end_logits\n  del model\n  del config\n  gc.collect()\n\n  return start_logits_concat, end_logits_concat","metadata":{"id":"3kh3b2E74vx0","execution":{"iopub.status.busy":"2021-11-14T20:12:05.104621Z","iopub.execute_input":"2021-11-14T20:12:05.105135Z","iopub.status.idle":"2021-11-14T20:12:05.115738Z","shell.execute_reply.started":"2021-11-14T20:12:05.1051Z","shell.execute_reply":"2021-11-14T20:12:05.114997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaned_predictions(df):\n  # cleaned predictions\n  bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\n  bad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\n  tamil_ad = \"கி.பி\"\n  tamil_bc = \"கி.மு\"\n  tamil_km = \"கி.மீ\"\n  hindi_ad = \"ई\"\n  hindi_bc = \"ई.पू\"\n\n\n  cleaned_preds = []\n  for pred, context in df[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)\n  return cleaned_preds\n","metadata":{"id":"z4KFkOHDFuJi","execution":{"iopub.status.busy":"2021-11-14T20:12:05.117089Z","iopub.execute_input":"2021-11-14T20:12:05.117375Z","iopub.status.idle":"2021-11-14T20:12:05.128549Z","shell.execute_reply.started":"2021-11-14T20:12:05.117344Z","shell.execute_reply":"2021-11-14T20:12:05.127789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the jaccard function\ndef jaccard(row):\n  intersect=set(row[0].lower().split()).intersection(set(row[1].lower().split()))\n  union=set(row[0].lower().split()).union(set(row[1].lower().split()))\n\n  return len(intersect)/len(union)","metadata":{"id":"HYRtyVduHH3a","execution":{"iopub.status.busy":"2021-11-14T20:12:05.130069Z","iopub.execute_input":"2021-11-14T20:12:05.130355Z","iopub.status.idle":"2021-11-14T20:12:05.139168Z","shell.execute_reply.started":"2021-11-14T20:12:05.130323Z","shell.execute_reply":"2021-11-14T20:12:05.138493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accelerator=Accelerator()\n#tokenizer=AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)  \n#checkpoints_dir=['../input/chaiikfold0',\n #              '../input/chaiikfold1','../input/chaiikfold2/fold2',\n  #              '../input/chaiikfold3/fold3','../input/chaiikfold4/fold4',\n   #             '../input/chaiikfold5/fold5']\n#jaccard_score=[]\n#for i,checkpoint in enumerate(checkpoints_dir):\n #       valid_df=train_df[train_df['fold']==i]\n  #      tokenizer=AutoTokenizer.from_pretrained(checkpoint, use_fast=True) \n   #     valid_examples=d.Dataset.from_pandas(valid_df)\n    #    valid_dataset = valid_examples.map(\n     #       prepare_validation_features,\n      #      batched=True,\n       #     remove_columns=valid_examples.column_names\n        #)\n        #output_numpy=logit_predictions(valid_dataset,checkpoint,accelerator)\n        #final_predictions = postprocess_qa_predictions(valid_examples,valid_dataset,output_numpy)\n        #valid_df['PredictionString']=valid_df['id'].apply(lambda r: final_predictions[r])\n        #predictions=cleaned_predictions(valid_df)\n        #valid_df['PredictionString']=predictions\n        #valid_df['jaccard'] = valid_df[['answer_text', 'PredictionString']].apply(jaccard, axis=1)\n        #jaccard_score.append(valid_df['jaccard'].mean())\n        #del valid_dataset\n        #del valid_df\n        #del valid_examples\n        #del tokenizer\n        #gc.collect()","metadata":{"id":"bqbDE7TN1_-N","execution":{"iopub.status.busy":"2021-11-14T20:12:05.140169Z","iopub.execute_input":"2021-11-14T20:12:05.140431Z","iopub.status.idle":"2021-11-14T20:12:05.147747Z","shell.execute_reply.started":"2021-11-14T20:12:05.140399Z","shell.execute_reply":"2021-11-14T20:12:05.147058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accelerator=Accelerator()\nfold=0\n#tokenizer=AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)  \ncheckpoints_dir='../input/chaiikfold000/fold00'\nvalid_df=train_df[train_df['fold']==fold]\ntokenizer=AutoTokenizer.from_pretrained(checkpoints_dir, use_fast=True) \nvalid_examples=d.Dataset.from_pandas(valid_df)\nvalid_dataset = valid_examples.map(\n            prepare_validation_features,\n            batched=True,\n            remove_columns=valid_examples.column_names\n        )\noutput_numpy=logit_predictions(valid_dataset,checkpoints_dir,accelerator)\nfinal_predictions = postprocess_qa_predictions(valid_examples,valid_dataset,output_numpy)\nvalid_df['PredictionString']=valid_df['id'].apply(lambda r: final_predictions[r])\npredictions=cleaned_predictions(valid_df)\nvalid_df['PredictionString']=predictions\nvalid_df['jaccard'] = valid_df[['answer_text', 'PredictionString']].apply(jaccard, axis=1)\nprint(f'Jaccard score for {fold}th fold:',valid_df['jaccard'].mean())\ndel valid_dataset\ndel valid_examples\ndel tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T20:12:05.158766Z","iopub.execute_input":"2021-11-14T20:12:05.159016Z","iopub.status.idle":"2021-11-14T20:15:56.90687Z","shell.execute_reply.started":"2021-11-14T20:12:05.158986Z","shell.execute_reply":"2021-11-14T20:15:56.906182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df[valid_df['jaccard']==0]","metadata":{"execution":{"iopub.status.busy":"2021-11-14T20:15:56.9082Z","iopub.execute_input":"2021-11-14T20:15:56.909027Z","iopub.status.idle":"2021-11-14T20:15:56.950367Z","shell.execute_reply.started":"2021-11-14T20:15:56.90899Z","shell.execute_reply":"2021-11-14T20:15:56.949515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Jaccard 6 fold score:',jaccard_score)\nprint('Stratified Kfold jaccard mean score:',np.array(jaccard_score).mean())","metadata":{"id":"nfElRq-F-WeR","execution":{"iopub.status.busy":"2021-11-09T15:57:56.953407Z","iopub.execute_input":"2021-11-09T15:57:56.953694Z","iopub.status.idle":"2021-11-09T15:57:56.958804Z","shell.execute_reply.started":"2021-11-09T15:57:56.953665Z","shell.execute_reply":"2021-11-09T15:57:56.958126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accelerator=Accelerator()\ntokenizer=AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)  \ncheckpoints_dir=['../input/chaiikfold123456/fold0/fold0',\n               '../input/chaiikfold123456/fold1/fold1','../input/chaiikfold123456/fold2/fold2',\n                '../input/chaiikfold123456/fold3/fold3','../input/chaiikfold123456/fold4/fold4']\ntrain_examples=d.Dataset.from_pandas(train_df)\ntrain_dataset = train_examples.map(\n           prepare_validation_features,\n            batched=True,\n           remove_columns=train_examples.column_names\n        )\n    \n\noutput_numpy=[0.0,0.0]\nfor checkpoint in checkpoints_dir:\n     output=logit_predictions(train_dataset,checkpoint,accelerator)\n     output_numpy[0]+=output[0]\n     output_numpy[1]+=output[1]\noutput_numpy=(output_numpy[0]/6,output_numpy[1]/6)\nfinal_predictions = postprocess_qa_predictions(train_examples,train_dataset,output_numpy)\ntrain_df['PredictionString']=train_df['id'].apply(lambda r: final_predictions[r])\npredictions=cleaned_predictions(train_df)\ntrain_df['PredictionString']=predictions\ntrain_df['jaccard'] = train_df[['answer_text', 'PredictionString']].apply(jaccard, axis=1)\nprint('Jaccard score on train set:',train_df['jaccard'].mean())\ndel train_dataset\ndel train_examples\ngc.collect()\n","metadata":{"id":"7pd7VHd0BzJt","execution":{"iopub.status.busy":"2021-11-08T20:13:07.344843Z","iopub.execute_input":"2021-11-08T20:13:07.34521Z","iopub.status.idle":"2021-11-08T20:33:04.841567Z","shell.execute_reply.started":"2021-11-08T20:13:07.345106Z","shell.execute_reply":"2021-11-08T20:33:04.840531Z"},"trusted":true},"execution_count":null,"outputs":[]}]}