{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/huggingface-accelerate')\nsys.path.append('../input/transformershuggingface/transformer_repo')","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:31:55.446162Z","iopub.execute_input":"2021-11-14T16:31:55.446521Z","iopub.status.idle":"2021-11-14T16:31:55.543325Z","shell.execute_reply.started":"2021-11-14T16:31:55.446436Z","shell.execute_reply":"2021-11-14T16:31:55.542636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install madgrad","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:31:55.545123Z","iopub.execute_input":"2021-11-14T16:31:55.545388Z","iopub.status.idle":"2021-11-14T16:32:04.054422Z","shell.execute_reply.started":"2021-11-14T16:31:55.545354Z","shell.execute_reply":"2021-11-14T16:32:04.053466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint='../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:04.056228Z","iopub.execute_input":"2021-11-14T16:32:04.056509Z","iopub.status.idle":"2021-11-14T16:32:04.062739Z","shell.execute_reply.started":"2021-11-14T16:32:04.056472Z","shell.execute_reply":"2021-11-14T16:32:04.061973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:04.065588Z","iopub.execute_input":"2021-11-14T16:32:04.066282Z","iopub.status.idle":"2021-11-14T16:32:13.01375Z","shell.execute_reply.started":"2021-11-14T16:32:04.066245Z","shell.execute_reply":"2021-11-14T16:32:13.012921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport os\nimport sys\nfrom IPython.display import FileLink\n%env WANDB_DISABLED=True\nimport torch\nimport datasets as d\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader\nfrom src.accelerate import Accelerator\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport math\nimport os\nimport random\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    default_data_collator,\n    get_scheduler,\n    set_seed)\nfrom madgrad import MADGRAD","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:13.015938Z","iopub.execute_input":"2021-11-14T16:32:13.016507Z","iopub.status.idle":"2021-11-14T16:32:23.70908Z","shell.execute_reply.started":"2021-11-14T16:32:13.016456Z","shell.execute_reply":"2021-11-14T16:32:23.708342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the seed\nseed=124\nrandom.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:23.710486Z","iopub.execute_input":"2021-11-14T16:32:23.710792Z","iopub.status.idle":"2021-11-14T16:32:23.721724Z","shell.execute_reply.started":"2021-11-14T16:32:23.710754Z","shell.execute_reply":"2021-11-14T16:32:23.720862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load  data\nimport pandas as pd\ntrain_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nsubmission_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsquad=pd.read_csv('../input/squadhindi/xquad.csv')\nmlqa=pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:23.723716Z","iopub.execute_input":"2021-11-14T16:32:23.724353Z","iopub.status.idle":"2021-11-14T16:32:24.904894Z","shell.execute_reply.started":"2021-11-14T16:32:23.724308Z","shell.execute_reply":"2021-11-14T16:32:24.904121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"squad['id']=['sar'+str(item) for item in range(5427,5427+squad.shape[0])]\nmlqa['id']=['sar'+str(item) for item in range(mlqa.shape[0])]\n# merge train_df and squad\ntrain_df=pd.concat([train_df,squad,mlqa])","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:24.90676Z","iopub.execute_input":"2021-11-14T16:32:24.907022Z","iopub.status.idle":"2021-11-14T16:32:24.931588Z","shell.execute_reply.started":"2021-11-14T16:32:24.906988Z","shell.execute_reply":"2021-11-14T16:32:24.930723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_answers_as_dict(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }\n\ntrain_df['answers'] = train_df[['answer_start', 'answer_text']].apply(get_answers_as_dict, axis=1)\n\ntrain_df.reset_index(drop=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:24.932655Z","iopub.execute_input":"2021-11-14T16:32:24.932893Z","iopub.status.idle":"2021-11-14T16:32:25.040566Z","shell.execute_reply.started":"2021-11-14T16:32:24.93286Z","shell.execute_reply":"2021-11-14T16:32:25.039831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset into 6 folds\ntrain_df['fold']=-99\nskf=StratifiedKFold(n_splits=6,random_state=seed,shuffle=True)\nfor i,(train_index, test_index) in enumerate(skf.split(train_df, train_df['language'])):\n    train_df.loc[test_index,'fold']=i\n\n#train_df.to_csv('train_df.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:25.043723Z","iopub.execute_input":"2021-11-14T16:32:25.044021Z","iopub.status.idle":"2021-11-14T16:32:25.068231Z","shell.execute_reply.started":"2021-11-14T16:32:25.043971Z","shell.execute_reply":"2021-11-14T16:32:25.067293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper parameters\n#gradient_accumulation_steps = 2\nmax_length = 384\ndoc_stride = 128\n# train\nnum_train_epochs = 1\ntrain_batch_size = 4\neval_batch_size = 8\n# optimizer\n#learning_rate = 1.5e-5\n# weight_decay = 1e-2\n#epsilon=1e-8\n# scheduler\n#scheduler= 'linear'\nwarmup_ratio = 0.1\n#num_warmup_steps=0\n# evaluate\noutput_dir = 'output'","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:25.069408Z","iopub.execute_input":"2021-11-14T16:32:25.069689Z","iopub.status.idle":"2021-11-14T16:32:25.074982Z","shell.execute_reply.started":"2021-11-14T16:32:25.069656Z","shell.execute_reply":"2021-11-14T16:32:25.073915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] !=1 :\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:25.076349Z","iopub.execute_input":"2021-11-14T16:32:25.076907Z","iopub.status.idle":"2021-11-14T16:32:25.092089Z","shell.execute_reply.started":"2021-11-14T16:32:25.076871Z","shell.execute_reply":"2021-11-14T16:32:25.09135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data,fold,model,config,tokenizer,output_dir,learning_rate=1.5e-5,\n          gradient_accumulation_steps = 2,scheduler= 'linear',\n         num_warmup_steps=0,\n         epsilon=1e-8,correct_bias=True,\n         no_decay = [\"bias\", \"LayerNorm.weight\"],\n         weight_decay = 1e-2):\n    accelerator = Accelerator()\n    data=data[data['fold']!=fold]\n    data.drop('language',axis=1,inplace=True)\n    data=d.Dataset.from_pandas(data)\n    data= data.map(prepare_train_features, batched=True,remove_columns=data.column_names)\n    train_dataloader = DataLoader(\n        data, shuffle=True, collate_fn= default_data_collator, batch_size=train_batch_size)\n    del data\n    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.',\n                   'layer.5.','layer.6.','layer.7.','layer.8.','layer.9.',\n                   'layer.10.','layer.11.']\n    optimizer_grouped_parameters = [\n        #{\n           # \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n           # \"weight_decay\":weight_decay,\n       # },\n        #{\n           # \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n           # \"weight_decay\": 0.0,\n        #},\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],\n         'weight_decay': weight_decay},\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],\n             'weight_decay':weight_decay, 'lr': learning_rate/2},\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],\n             'weight_decay':weight_decay, 'lr': learning_rate},\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],\n             'weight_decay':weight_decay, 'lr': learning_rate*2},\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],\n             'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],\n             'weight_decay': 0.0, 'lr': learning_rate/2},\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],\n             'weight_decay': 0.0, 'lr': learning_rate},\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],\n             'weight_decay': 0.0, 'lr': learning_rate*2}\n         ]\n\n\n    optimizer =AdamW(optimizer_grouped_parameters, lr=learning_rate,\n                     eps=epsilon,weight_decay=weight_decay)\n\n    # Prepare everything with the `accelerator`.\n    model, optimizer, train_dataloader= accelerator.prepare(\n        model, optimizer, train_dataloader)    \n  \n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n\n    lr_scheduler = get_scheduler(\n        name=scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=max_train_steps)\n    #swa_model = AveragedModel(model)\n    #scheduler = CosineAnnealingLR(optimizer, T_max=100)\n    #swa_start = 1\n    #swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n   \n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps=0\n\n    for epoch in range(num_train_epochs):\n        model.train()\n        \n        #if epoch==1:\n            #swa_model.update_parameters(model)\n            #swa_scheduler.step()\n            #break # break the loop\n            \n        for step, batch in enumerate(train_dataloader):\n            for key,value in batch.items():\n                batch[key]=value.to(device)\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss/gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if completed_steps >= max_train_steps:\n                break\n    # Update bn statistics for the swa_model at the end\n    #torch.optim.swa_utils.update_bn(train_dataloader, swa_model)\n        \n    output_dir=os.path.join(output_dir,f\"checkpoint_fold-{fold}\")\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n    config.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    del model\n    #del swa_model\n    del tokenizer\n    del config\n    del train_dataloader\n    del optimizer\n    del accelerator\n    gc.collect()\n    return \n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:25.093571Z","iopub.execute_input":"2021-11-14T16:32:25.094013Z","iopub.status.idle":"2021-11-14T16:32:25.108636Z","shell.execute_reply.started":"2021-11-14T16:32:25.093974Z","shell.execute_reply":"2021-11-14T16:32:25.107798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def l2_regularizer(weight_decay):\n #   def regularizer(model):\n  #      l2 = 0.0\n  #      for p in model.parameters():\n   #         l2 += torch.sqrt(torch.sum(p ** 2))\n    #    return 0.5 * weight_decay * l2\n    #return regularizer\n\n#def cyclic_learning_rate(epoch, cycle, alpha_1, alpha_2):\n #   def schedule(iter):\n  #      t = ((epoch % cycle) + iter) / cycle\n   #     if t < 0.5:\n    #        return alpha_1 * (1.0 - 2.0 * t) + alpha_2 * 2.0 * t\n     #   else:\n      #      return alpha_1 * (2.0 * t - 1.0) + alpha_2 * (2.0 - 2.0 * t)\n    #return schedule\n\n\n#def adjust_learning_rate(optimizer, lr):\n #   for param_group in optimizer.param_groups:\n  #      param_group['lr'] = lr\n   # return lr","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:25.110026Z","iopub.execute_input":"2021-11-14T16:32:25.110356Z","iopub.status.idle":"2021-11-14T16:32:25.121639Z","shell.execute_reply.started":"2021-11-14T16:32:25.110316Z","shell.execute_reply":"2021-11-14T16:32:25.120853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def train(data,fold,model,config,tokenizer,output_dir,learning_rate=1.5e-5,cycle=4, lr_1=1.5e-5, lr_2=1.5e-6,\n #         gradient_accumulation_steps = 2, regularizer=None,\n  #       epsilon=1e-8,\n   #      no_decay = [\"bias\", \"LayerNorm.weight\"],\n    #     weight_decay = 1e-4):\n    #accelerator = Accelerator()\n    #data=data[data['fold']!=fold]\n    #data.drop('language',axis=1,inplace=True)\n    #data=d.Dataset.from_pandas(data)\n    #data= data.map(prepare_train_features, batched=True,remove_columns=data.column_names)\n    #train_dataloader = DataLoader(\n     #   data, shuffle=True, collate_fn= default_data_collator, batch_size=train_batch_size)\n    #del data\n    #group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    #group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    #group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    #group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.',\n     #              'layer.5.','layer.6.','layer.7.','layer.8.','layer.9.',\n      #             'layer.10.','layer.11.']\n    #optimizer_grouped_parameters = [\n        #{\n           # \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n           # \"weight_decay\":weight_decay,\n       # },\n        #{\n           # \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n           # \"weight_decay\": 0.0,\n        #},\n     #   {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],\n      #   'weight_decay': weight_decay},\n       #     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],\n        #     'weight_decay':weight_decay, 'lr': learning_rate/2},\n         #   {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],\n          #   'weight_decay':weight_decay, 'lr': learning_rate},\n           # {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],\n            # 'weight_decay':weight_decay, 'lr': learning_rate*2},\n            #{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],\n            # 'weight_decay': 0.0},\n            #{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],\n            # 'weight_decay': 0.0, 'lr': learning_rate/2},\n            #{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],\n            # 'weight_decay': 0.0, 'lr': learning_rate},\n            #{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],\n            # 'weight_decay': 0.0, 'lr': learning_rate*2}\n         #]\n\n\n    #optimizer =AdamW(optimizer_grouped_parameters, lr=learning_rate,\n     #                eps=epsilon,weight_decay=weight_decay)\n#\n #   # Prepare everything with the `accelerator`.\n  #  model, optimizer, train_dataloader= accelerator.prepare(\n   #     model, optimizer, train_dataloader)    \n  #\n   # num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    #max_train_steps = num_train_epochs * num_update_steps_per_epoch\n    \n    # Only show the progress bar once on each machine.\n    #progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    #completed_steps=0\n    \n    #for epoch in range(num_train_epochs):\n     #   model.train()\n      #  lr_schedule =cyclic_learning_rate(epoch, cycle, lr_1, lr_2)\n        \n       # for step, batch in enumerate(train_dataloader):\n        #    for key,value in batch.items():\n         #       batch[key]=value.to(device)\n            \n            \n          #  outputs = model(**batch)\n           # loss = outputs.loss\n            #loss = loss/gradient_accumulation_steps\n            #if regularizer is not None:\n             #   loss += regularizer(model)\n            \n            #accelerator.backward(loss)\n            #if step % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n             #   optimizer.step()\n             #   lr = lr_schedule(step/max_train_steps)\n              #  adjust_learning_rate(optimizer, lr)\n                    \n               # optimizer.zero_grad()\n                #progress_bar.update(1)\n                #completed_steps += 1\n\n            #if completed_steps >= max_train_steps:\n             #   break\n                \n    #output_dir=os.path.join(output_dir,f\"checkpoint_fold-{fold}\")\n    #if not os.path.exists(output_dir):\n     #   os.makedirs(output_dir)\n    #torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n    #config.save_pretrained(output_dir)\n    #tokenizer.save_pretrained(output_dir)\n    #del model\n    #del tokenizer\n    #del config\n    #del train_dataloader\n    #del optimizer\n    #del accelerator\n    #gc.collect()\n    #return ","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:25.123117Z","iopub.execute_input":"2021-11-14T16:32:25.123414Z","iopub.status.idle":"2021-11-14T16:32:25.151443Z","shell.execute_reply.started":"2021-11-14T16:32:25.12338Z","shell.execute_reply":"2021-11-14T16:32:25.150693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_checkpoint)\ntokenizer=AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint,config=config)\ndevice=torch.device(\"cuda\")\nmodel.to(device)\nd3='s'","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:25.152755Z","iopub.execute_input":"2021-11-14T16:32:25.153279Z","iopub.status.idle":"2021-11-14T16:32:53.708434Z","shell.execute_reply.started":"2021-11-14T16:32:25.153241Z","shell.execute_reply":"2021-11-14T16:32:53.707646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for n,p in model.named_parameters():\n    #print(n)\n    #print(\"**********************\")","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:53.709722Z","iopub.execute_input":"2021-11-14T16:32:53.710003Z","iopub.status.idle":"2021-11-14T16:32:53.714717Z","shell.execute_reply.started":"2021-11-14T16:32:53.709958Z","shell.execute_reply":"2021-11-14T16:32:53.713787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#weight_decay=1e-3\n#regularizer=l2_regularizer(weight_decay)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:53.716387Z","iopub.execute_input":"2021-11-14T16:32:53.716765Z","iopub.status.idle":"2021-11-14T16:32:53.724769Z","shell.execute_reply.started":"2021-11-14T16:32:53.716731Z","shell.execute_reply":"2021-11-14T16:32:53.723974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train(train_df,0,model,config,tokenizer,output_dir,learning_rate=1.5e-5,\n #         gradient_accumulation_steps = 2,scheduler= 'linear',\n  #      num_warmup_steps=0,epsilon=1e-8,correct_bias=True,no_decay = [\"bias\", \"LayerNorm.weight\"],\n   #   weight_decay = 1e-2)\n#FileLink(r\"./output/checkpoint_fold-0/pytorch_model.bin\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T16:32:53.727076Z","iopub.execute_input":"2021-11-14T16:32:53.72771Z","iopub.status.idle":"2021-11-14T17:22:39.721803Z","shell.execute_reply.started":"2021-11-14T16:32:53.727649Z","shell.execute_reply":"2021-11-14T17:22:39.720989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train(train_df,1,model,config,tokenizer,output_dir,learning_rate=1.4e-5,\n #        gradient_accumulation_steps = 2,scheduler= 'cosine',\n  #      num_warmup_steps=0,epsilon=1e-8,correct_bias=True,no_decay = [\"bias\", \"LayerNorm.weight\"],\n   #     weight_decay = 1e-2)\n#FileLink(r\"./output/checkpoint_fold-1/pytorch_model.bin\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T17:22:39.723249Z","iopub.execute_input":"2021-11-14T17:22:39.723538Z","iopub.status.idle":"2021-11-14T17:22:39.72758Z","shell.execute_reply.started":"2021-11-14T17:22:39.723484Z","shell.execute_reply":"2021-11-14T17:22:39.726656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train(train_df,2,model,config,tokenizer,output_dir,learning_rate=1.5e-5,\n  #      gradient_accumulation_steps = 2,scheduler= 'linear',\n   #      num_warmup_steps=0,epsilon=1e-8,correct_bias=True,no_decay = [\"bias\", \"LayerNorm.weight\"],\n    #    weight_decay = 1e-4)\n#FileLink(r\"./output/checkpoint_fold-2/pytorch_model.bin\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T17:22:39.729097Z","iopub.execute_input":"2021-11-14T17:22:39.72936Z","iopub.status.idle":"2021-11-14T17:22:40.444803Z","shell.execute_reply.started":"2021-11-14T17:22:39.729324Z","shell.execute_reply":"2021-11-14T17:22:40.443811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train(train_df,3,model,config,tokenizer,output_dir,learning_rate=1.5e-5,\n   # gradient_accumulation_steps = 2,scheduler= 'cosine',\n   # num_warmup_steps=0,epsilon=1e-8,correct_bias=True,no_decay = [\"bias\", \"LayerNorm.weight\"],\n  #  weight_decay = 1e-2)\n#FileLink(r\"./output/checkpoint_fold-3/pytorch_model.bin\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T17:22:40.446378Z","iopub.execute_input":"2021-11-14T17:22:40.446691Z","iopub.status.idle":"2021-11-14T17:22:41.21101Z","shell.execute_reply.started":"2021-11-14T17:22:40.44665Z","shell.execute_reply":"2021-11-14T17:22:41.210045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train with learning rate divided by 2\n#train(train_df,4,model,config,tokenizer,output_dir,learning_rate=1.5e-5,\n #        gradient_accumulation_steps = 2,scheduler= 'linear',\n  #       num_warmup_steps=0,epsilon=1e-8,correct_bias=True,no_decay = [\"bias\", \"LayerNorm.weight\"],\n   #  weight_decay = 1e-2)\n#FileLink(r\"./output/checkpoint_fold-4/pytorch_model.bin\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T17:22:41.212184Z","iopub.execute_input":"2021-11-14T17:22:41.212531Z","iopub.status.idle":"2021-11-14T17:22:44.492872Z","shell.execute_reply.started":"2021-11-14T17:22:41.212489Z","shell.execute_reply":"2021-11-14T17:22:44.491978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train(train_df,5,model,config,tokenizer,output_dir,learning_rate=1.5e-5,\n #         gradient_accumulation_steps = 2,scheduler= 'linear',\n  #       num_warmup_steps=2,epsilon=1e-8,correct_bias=True,no_decay = [\"bias\", \"LayerNorm.weight\"],\n   #  weight_decay = 1e-2)\n#FileLink(r\"./output/checkpoint_fold-5/pytorch_model.bin\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T17:22:44.494193Z","iopub.execute_input":"2021-11-14T17:22:44.494597Z","iopub.status.idle":"2021-11-14T17:22:50.476726Z","shell.execute_reply.started":"2021-11-14T17:22:44.494518Z","shell.execute_reply":"2021-11-14T17:22:50.475749Z"},"trusted":true},"execution_count":null,"outputs":[]}]}