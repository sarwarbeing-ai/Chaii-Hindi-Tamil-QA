{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/huggingface-accelerate')\nsys.path.append('../input/transformershuggingface/transformer_repo')","metadata":{"id":"ZBKvQdjJ1_99","execution":{"iopub.status.busy":"2021-11-10T06:12:12.57237Z","iopub.execute_input":"2021-11-10T06:12:12.572688Z","iopub.status.idle":"2021-11-10T06:12:12.670707Z","shell.execute_reply.started":"2021-11-10T06:12:12.572609Z","shell.execute_reply":"2021-11-10T06:12:12.670022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint='../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'","metadata":{"id":"zCAL3UJm1_-D","execution":{"iopub.status.busy":"2021-11-10T06:12:12.672345Z","iopub.execute_input":"2021-11-10T06:12:12.672814Z","iopub.status.idle":"2021-11-10T06:12:12.676508Z","shell.execute_reply.started":"2021-11-10T06:12:12.672781Z","shell.execute_reply":"2021-11-10T06:12:12.675875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq","metadata":{"id":"ImW8Iodz1_-F","execution":{"iopub.status.busy":"2021-11-10T06:12:12.677903Z","iopub.execute_input":"2021-11-10T06:12:12.678357Z","iopub.status.idle":"2021-11-10T06:12:21.99133Z","shell.execute_reply.started":"2021-11-10T06:12:12.678325Z","shell.execute_reply":"2021-11-10T06:12:21.990498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport collections\nimport pandas as pd\nimport gc\nimport os\nimport sys\nfrom IPython.display import FileLink\n%env WANDB_DISABLED=True\nimport torch\nimport datasets as d\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader\nfrom src.accelerate import Accelerator\nimport math\nimport os\nimport random\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    default_data_collator,\n    get_scheduler,\n    SchedulerType,\n    set_seed)\n","metadata":{"id":"bwFxs1kk1_-F","execution":{"iopub.status.busy":"2021-11-10T06:12:21.993501Z","iopub.execute_input":"2021-11-10T06:12:21.993713Z","iopub.status.idle":"2021-11-10T06:12:32.467534Z","shell.execute_reply.started":"2021-11-10T06:12:21.993689Z","shell.execute_reply":"2021-11-10T06:12:32.466808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the seed\nseed=124\nrandom.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nset_seed(seed)","metadata":{"id":"hwRZX8p-1_-G","execution":{"iopub.status.busy":"2021-11-10T06:12:32.468688Z","iopub.execute_input":"2021-11-10T06:12:32.468963Z","iopub.status.idle":"2021-11-10T06:12:32.479878Z","shell.execute_reply.started":"2021-11-10T06:12:32.468932Z","shell.execute_reply":"2021-11-10T06:12:32.478986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load  data\nimport pandas as pd\ntrain_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nsubmission_df=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\nsquad=pd.read_csv('../input/squadhindi/xquad.csv')\nmlqa=pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')","metadata":{"id":"l5xw8bYt1_-H","execution":{"iopub.status.busy":"2021-11-10T06:12:32.481167Z","iopub.execute_input":"2021-11-10T06:12:32.482135Z","iopub.status.idle":"2021-11-10T06:12:33.672688Z","shell.execute_reply.started":"2021-11-10T06:12:32.482013Z","shell.execute_reply":"2021-11-10T06:12:33.671976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"squad['id']=['sar'+str(item) for item in range(5427,5427+squad.shape[0])]\nmlqa['id']=['sar'+str(item) for item in range(mlqa.shape[0])]\n# merge train_df and squad\ntrain_df=pd.concat([train_df,squad,mlqa])","metadata":{"id":"_2veWPZx1_-I","execution":{"iopub.status.busy":"2021-11-10T06:12:33.674185Z","iopub.execute_input":"2021-11-10T06:12:33.674465Z","iopub.status.idle":"2021-11-10T06:12:33.697397Z","shell.execute_reply.started":"2021-11-10T06:12:33.674432Z","shell.execute_reply":"2021-11-10T06:12:33.696719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_answers_as_dict(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }\n\ntrain_df['answers'] = train_df[['answer_start', 'answer_text']].apply(get_answers_as_dict, axis=1)\n\ntrain_df.reset_index(drop=True,inplace=True)","metadata":{"id":"S5iIxzW51_-K","execution":{"iopub.status.busy":"2021-11-10T06:12:33.698674Z","iopub.execute_input":"2021-11-10T06:12:33.698925Z","iopub.status.idle":"2021-11-10T06:12:33.801932Z","shell.execute_reply.started":"2021-11-10T06:12:33.698894Z","shell.execute_reply":"2021-11-10T06:12:33.801162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper parameters\nmax_length = 384\ndoc_stride = 128\neval_batch_size = 8","metadata":{"id":"ZMgPugrX3RnT","execution":{"iopub.status.busy":"2021-11-10T06:12:33.803187Z","iopub.execute_input":"2021-11-10T06:12:33.803457Z","iopub.status.idle":"2021-11-10T06:12:33.807548Z","shell.execute_reply.started":"2021-11-10T06:12:33.803426Z","shell.execute_reply":"2021-11-10T06:12:33.806669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_test_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"id":"mVotVzf61_-L","execution":{"iopub.status.busy":"2021-11-10T06:12:33.811217Z","iopub.execute_input":"2021-11-10T06:12:33.811511Z","iopub.status.idle":"2021-11-10T06:12:33.821321Z","shell.execute_reply.started":"2021-11-10T06:12:33.811455Z","shell.execute_reply":"2021-11-10T06:12:33.820531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples,\n    features,\n    predictions,\n    n_best_size: int = 20,\n    max_answer_length: int = 30):\n    \n    assert len(predictions) == 2, \"`predictions` should be a tuple with two elements (start_logits, end_logits).\"\n    all_start_logits, all_end_logits = predictions\n\n    assert len(predictions[0]) == len(features), f\"Got {len(predictions[0])} predictions and {len(features)} features.\"\n\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    all_predictions = collections.OrderedDict()\n    \n    \n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        prelim_predictions = []\n\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n            # available in the current feature.\n            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n            \n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    # Don't consider answer that don't have the maximum context available (if such information is\n                    # provided).\n                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n                        continue\n                    prelim_predictions.append(\n                        {\n                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"start_logit\": start_logits[start_index],\n                            \"end_logit\": end_logits[end_index],\n                        }\n                    )\n        # Only keep the best `n_best_size` predictions.\n        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n\n        # Use the offsets to gather the answer text in the original context.\n        context = example[\"context\"]\n        for pred in predictions:\n            offsets = pred.pop(\"offsets\")\n            pred[\"text\"] = context[offsets[0] : offsets[1]]\n\n        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n        # failure.\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n\n        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n        # the LogSumExp trick).\n        scores = np.array([pred.pop(\"score\") for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n\n        # Include the probabilities in our predictions.\n        for prob, pred in zip(probs, predictions):\n            pred[\"probability\"] = prob\n\n        # Pick the best prediction. If the null answer is not possible, this is easy.\n        all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n    return all_predictions","metadata":{"id":"y7JGMlMf3oNz","execution":{"iopub.status.busy":"2021-11-10T06:12:33.82286Z","iopub.execute_input":"2021-11-10T06:12:33.823126Z","iopub.status.idle":"2021-11-10T06:12:33.848291Z","shell.execute_reply.started":"2021-11-10T06:12:33.823095Z","shell.execute_reply":"2021-11-10T06:12:33.847577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\ndef create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n\n        step = 0\n        # create a numpy array and fill it with -100.\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather\n        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n            # And after every iteration we have to change the step\n\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n\n            if step + batch_size < len(dataset):\n                logits_concat[step : step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n\n            step += batch_size\n\n        return logits_concat","metadata":{"id":"DwhfdjlL1_-N","execution":{"iopub.status.busy":"2021-11-10T06:12:33.849776Z","iopub.execute_input":"2021-11-10T06:12:33.850398Z","iopub.status.idle":"2021-11-10T06:12:33.861239Z","shell.execute_reply.started":"2021-11-10T06:12:33.850363Z","shell.execute_reply":"2021-11-10T06:12:33.860543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logit_predictions(test_data,checkpoint,accelerator):\n  config = AutoConfig.from_pretrained(checkpoint)\n  model = AutoModelForQuestionAnswering.from_pretrained(checkpoint,config=config)\n  device=torch.device(\"cuda\")\n  model.to(device)\n  test_dataset_for_model = test_data.remove_columns([\"example_id\", \"offset_mapping\"])\n  test_dataloader = DataLoader(\n            test_dataset_for_model, collate_fn=default_data_collator, batch_size=eval_batch_size)\n\n  all_start_logits = []\n  all_end_logits = []\n  for step, batch in enumerate(tqdm(test_dataloader)):\n        with torch.no_grad():\n            for key,value in batch.items():\n                batch[key]=value.to(device)\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n\n                \n            all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n\n  max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n  # concatenate the numpy array\n  start_logits_concat = create_and_fill_np_array(all_start_logits, test_data, max_len)\n  end_logits_concat = create_and_fill_np_array(all_end_logits, test_data, max_len)\n\n  # delete the list of numpy arrays\n  del all_start_logits\n  del all_end_logits\n  del model\n  del config\n  gc.collect()\n\n  return start_logits_concat, end_logits_concat","metadata":{"id":"3kh3b2E74vx0","execution":{"iopub.status.busy":"2021-11-10T06:12:33.862968Z","iopub.execute_input":"2021-11-10T06:12:33.863198Z","iopub.status.idle":"2021-11-10T06:12:33.875777Z","shell.execute_reply.started":"2021-11-10T06:12:33.863175Z","shell.execute_reply":"2021-11-10T06:12:33.875093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaned predictions\ndef cleaned_predictions(test_df):\n    #test_df['PredictionString']=test_df['id'].apply(lambda r:final_predictions[r])\n    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\n    bad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\n    tamil_ad = \"கி.பி\"\n    tamil_bc = \"கி.மு\"\n    tamil_km = \"கி.மீ\"\n    hindi_ad = \"ई\"\n    hindi_bc = \"ई.पू\"\n\n\n    cleaned_preds =[]\n    for _,pred, context in test_df[[\"id\",\"PredictionString\", \"context\"]].to_numpy():\n        if pred == \"\":\n            cleaned_preds.append(pred)\n            continue\n        while any([pred.startswith(y) for y in bad_starts]):\n             pred = pred[1:]\n        while any([pred.endswith(y) for y in bad_endings]):\n            if pred.endswith(\"...\"):\n                pred = pred[:-3]\n            else:\n                pred = pred[:-1]\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n            pred = pred+\".\"\n        \n        cleaned_preds.append(pred)\n    return cleaned_preds\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T06:12:33.878482Z","iopub.execute_input":"2021-11-10T06:12:33.878724Z","iopub.status.idle":"2021-11-10T06:12:33.890512Z","shell.execute_reply.started":"2021-11-10T06:12:33.878695Z","shell.execute_reply":"2021-11-10T06:12:33.889857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_examples=d.Dataset.from_pandas(test_df)\naccelerator=Accelerator()\ncheckpoints_dir=['../input/chaiikfold0','../input/chaiikfold1',\n                '../input/chaiikfold2/fold2','../input/chaiikfold3/fold3',\n                '../input/chaiikfold4/fold4','../input/chaiikfold5/fold5']\noutput_numpy=[0.0,0.0]\n#weights=[0.23,0.16,0.16,0.20,0.25]\npredictions=[]\nfor checkpoint in checkpoints_dir:\n  tokenizer=AutoTokenizer.from_pretrained(checkpoint, use_fast=True)  \n  test_dataset = test_examples.map(\n            prepare_test_features,\n            batched=True,\n            remove_columns=test_examples.column_names\n        )\n  output=logit_predictions(test_dataset,checkpoint,accelerator)\n  output_numpy[0]+=output[0]\n  output_numpy[1]+=output[1]\n\n  del tokenizer\n  gc.collect()\noutput_numpy=(output_numpy[0]/6,output_numpy[1]/6)\nprediction=postprocess_qa_predictions(test_examples,test_dataset,output_numpy,30, 40)\ntest_df['PredictionString']=test_df['id'].apply(lambda r:prediction[r])\nfinal_predictions=cleaned_predictions(test_df)","metadata":{"id":"bqbDE7TN1_-N","execution":{"iopub.status.busy":"2021-11-10T06:12:33.892035Z","iopub.execute_input":"2021-11-10T06:12:33.892237Z","iopub.status.idle":"2021-11-10T06:16:24.392628Z","shell.execute_reply.started":"2021-11-10T06:12:33.892209Z","shell.execute_reply":"2021-11-10T06:16:24.391815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_predictions=[]\n#idx=test_df.id\n#for id in idx:\n #   pred=[]\n  #  for i in range(6):\n   #     d_ith_fold=predictions[i]\n    #    pred.append(d_ith_fold[id])\n    #unique_pred=set(pred)\n    #if len(unique_pred)==6:\n     #   final_predictions.append(random.choices([predictions[0][id],predictions[-1][id]],weights=[0.20,0.24]))\n        \n    #else:\n     #   pred,counts=np.unique(pred,return_index=False,return_inverse=False,return_counts=True)\n      #  final_predictions.append(pred[np.argmax(counts)])","metadata":{"execution":{"iopub.status.busy":"2021-11-10T06:16:24.394058Z","iopub.execute_input":"2021-11-10T06:16:24.394333Z","iopub.status.idle":"2021-11-10T06:16:24.403304Z","shell.execute_reply.started":"2021-11-10T06:16:24.394299Z","shell.execute_reply":"2021-11-10T06:16:24.402429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission file\nsub=pd.DataFrame({'id':test_df['id']})\nsub['PredictionString']=final_predictions\nsub","metadata":{"id":"w_arGnaY-NtX","execution":{"iopub.status.busy":"2021-11-10T06:16:24.404765Z","iopub.execute_input":"2021-11-10T06:16:24.405284Z","iopub.status.idle":"2021-11-10T06:16:24.427942Z","shell.execute_reply.started":"2021-11-10T06:16:24.405241Z","shell.execute_reply":"2021-11-10T06:16:24.427126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\",index=False)","metadata":{"id":"f3SUhXP-AFnU","execution":{"iopub.status.busy":"2021-11-10T06:16:24.429339Z","iopub.execute_input":"2021-11-10T06:16:24.429837Z","iopub.status.idle":"2021-11-10T06:16:24.437911Z","shell.execute_reply.started":"2021-11-10T06:16:24.429802Z","shell.execute_reply":"2021-11-10T06:16:24.43712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}